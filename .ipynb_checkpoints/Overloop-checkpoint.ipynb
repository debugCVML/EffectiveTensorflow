{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 基本的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80236723 -0.72615633  0.74884724]\n",
      " [-0.35499364  0.83317327 -1.39034001]\n",
      " [-2.80633639  2.81632086  0.7470597 ]]\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "# 用Numpy产生随机数向量，并进行向量点乘\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(size=[3,3])\n",
    "y = np.random.normal(size=[3,3])\n",
    "z = np.dot(x,y)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06076807 -2.09271359 -0.41827142]\n",
      " [-1.60172617  3.15638876  1.11673987]\n",
      " [-0.31115821  1.31234944  0.39941484]]\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "# 用TensorFlow产生两个随机矩阵，并计算两个矩阵的乘积\n",
    "\n",
    "import tensorflow as tf\n",
    "x = tf.random_normal([3,3])\n",
    "y = tf.random_normal([3,3])\n",
    "z = tf.matmul(x,y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 使用TensorFlow拟合曲线\n",
    "\n",
    "假设有这样一条曲线， f(x) = 5x^2 + 3，用它产生数据，还有一条这样的曲线g(x, w) = w0 x^2 + w1 x + w2，让两条曲线近似相等，计算出所最有的W。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb56c472ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.74912357],\n",
      "       [-0.0360656 ],\n",
      "       [ 5.31532383]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 引入包\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 记录不长\n",
    "show_step = 20\n",
    "# 记录损失序列\n",
    "all_loss = []\n",
    "\n",
    "# 两个占位符\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 权重系数用变量表示\n",
    "w = tf.get_variable(\"w\", shape=[3,1])\n",
    "\n",
    "# 合并出一个张量，\n",
    "f = tf.stack([tf.square(x),x, tf.ones_like(x)], 1)\n",
    "\n",
    "# 实际g函数的值（这里消除了维度维1的轴，进行张量压缩）\n",
    "yhat = tf.squeeze(tf.matmul(f,w),1)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n",
    "\n",
    "# 定义优化操作\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "# 定义一个函数产生数据\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "# 开启一个Session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 进行epoch=1000的迭代优化\n",
    "for i in range(1000):\n",
    "    # 产生数据\n",
    "    x_val, y_val = generate_data()\n",
    "    # 运行一次优化\n",
    "    _, loss_val = sess.run([train_op, loss], {x:x_val, y:y_val})\n",
    "    # 如果步长满足，则记录损失\n",
    "    if (i+1) % show_step == 0:\n",
    "        all_loss = np.append(all_loss, loss_val)\n",
    "\n",
    "# 画出损失变化曲线图\n",
    "x = np.arange(1,1001,show_step)\n",
    "plt.plot(x, all_loss, 'b', label = \"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(sess.run([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 线性拟合\n",
    "\n",
    "拟合出一条y = x * 2 + 3的直线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 99, cost is 0.438810259\n",
      "Epoch is 199, cost is 0.059446841\n",
      "Epoch is 299, cost is 0.008053469\n",
      "Epoch is 399, cost is 0.001090974\n",
      "Epoch is 499, cost is 0.000147801\n",
      "Epoch is 599, cost is 0.000020018\n",
      "Epoch is 699, cost is 0.000002707\n",
      "Epoch is 799, cost is 0.000000368\n",
      "Epoch is 899, cost is 0.000000045\n",
      "Epoch is 999, cost is 0.000000022\n",
      "Epoch is 1099, cost is 0.000000022\n",
      "Epoch is 1199, cost is 0.000000022\n",
      "Epoch is 1299, cost is 0.000000022\n",
      "Epoch is 1399, cost is 0.000000022\n",
      "Epoch is 1499, cost is 0.000000022\n",
      "Epoch is 1599, cost is 0.000000022\n",
      "Epoch is 1699, cost is 0.000000022\n",
      "Epoch is 1799, cost is 0.000000022\n",
      "Epoch is 1899, cost is 0.000000022\n",
      "Epoch is 1999, cost is 0.000000022\n",
      "Trainning Finished!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb56ee9ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "3.60206\n"
     ]
    }
   ],
   "source": [
    "# 重置计算图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 迭代次数\n",
    "epoches = 2000\n",
    "show_step = 100\n",
    "\n",
    "# 训练数据\n",
    "xValue = np.arange(-10,10,1)\n",
    "yValue = xValue *2 + 3 + np.random.uniform(-10,10)\n",
    "\n",
    "# 记录样本总数\n",
    "nSamples = xValue.size\n",
    "\n",
    "X = tf.placeholder(dtype = tf.float32)\n",
    "Y = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "# 定义权值和偏置\n",
    "W = tf.Variable(np.random.randn())\n",
    "b = tf.Variable(np.random.randn())\n",
    "\n",
    "# 定义预测\n",
    "pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * nSamples)\n",
    "\n",
    "# 定义学习率\n",
    "learningRate = 0.01;\n",
    "# 定义训练器和优化操作\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learningRate)\n",
    "optimizer = trainer.minimize(loss)\n",
    "\n",
    "# 定义初始化全局变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 记录所有的损失\n",
    "all_loss = []\n",
    "\n",
    "# 在Session中运行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        for (x, y) in zip(xValue, yValue):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "        \n",
    "        if (epoch + 1) % show_step == 0:\n",
    "            loss_val = sess.run(loss, feed_dict={X:xValue, Y:yValue})\n",
    "            all_loss = np.append(all_loss, loss_val)\n",
    "            print(\"Epoch is {}, cost is {:.9f}\".format(epoch, loss_val))\n",
    "            \n",
    "    print(\"Trainning Finished!\")\n",
    "    # 画图\n",
    "    x = np.arange(1,epoches +1, show_step)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(xValue, yValue, 'ro', label = \"Original Data\")\n",
    "    plt.plot(xValue, sess.run(pred, feed_dict={X:xValue}), label = \"fitted Line\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(x, all_loss, 'b', label =\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(sess.run(W))\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "上面两个都是回归的例子，但是细心一点就会发现，第一个例子使用的是`tf.matmul`第二个例子是使用的是哟`tf.multiply`，这两个不能混用，看看两个函数的定义：\n",
    "\n",
    "1. tf.multiply\n",
    "定义如下\n",
    "```\n",
    "tf.multiply(\n",
    "    x,\n",
    "    y,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "其中x和y都是两个张量，并没有限制x和y的rank是多少。其中那个name是可选参数，用于定义操作的名称，在日志中显示中会有用。\n",
    "\n",
    "2. tf.matmul\n",
    "定义如下：\n",
    "```\n",
    "tf.matmul(\n",
    "    a,\n",
    "    b,\n",
    "    transpose_a=False,\n",
    "    transpose_b=False,\n",
    "    adjoint_a=False,\n",
    "    adjoint_b=False,\n",
    "    a_is_sparse=False,\n",
    "    b_is_sparse=False,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "明显比简单的两个张量乘法函数multiply复杂的多，这个函数要求两个参数张量的rank必须是大于等于2，主要是对两个张量的内两个维度进行乘法运算。另外还支持系数张量的运算。下面的一个小例子演示一下。rank为2和3时函数的输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is 2-d\n",
      "[[ 58  64]\n",
      " [139 154]]\n",
      "\n",
      "\n",
      "This is 3-d\n",
      "[[[ 94 100]\n",
      "  [229 244]]\n",
      "\n",
      " [[508 532]\n",
      "  [697 730]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 2-D tensor `a`\n",
    "# [[1, 2, 3],\n",
    "#  [4, 5, 6]]\n",
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "\n",
    "# 2-D tensor `b`\n",
    "# [[ 7,  8],\n",
    "#  [ 9, 10],\n",
    "#  [11, 12]]\n",
    "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
    "\n",
    "# `a` * `b`\n",
    "# [[ 58,  64],\n",
    "#  [139, 154]]\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(\"This is 2-d\")\n",
    "print(sess.run(c))\n",
    "\n",
    "# 3-D tensor `a`\n",
    "# [[[ 1,  2,  3],\n",
    "#   [ 4,  5,  6]],\n",
    "#  [[ 7,  8,  9],\n",
    "#   [10, 11, 12]]]\n",
    "a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
    "                shape=[2, 2, 3])\n",
    "\n",
    "# 3-D tensor `b`\n",
    "# [[[13, 14],\n",
    "#   [15, 16],\n",
    "#   [17, 18]],\n",
    "#  [[19, 20],\n",
    "#   [21, 22],\n",
    "#   [23, 24]]]\n",
    "b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
    "                shape=[2, 3, 2])\n",
    "\n",
    "# `a` * `b`\n",
    "# [[[ 94, 100],\n",
    "#   [229, 244]],\n",
    "#  [[508, 532],\n",
    "#   [697, 730]]]\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"This is 3-d\")\n",
    "print(sess.run(c))\n",
    "\n",
    "# Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
    "# In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
    "# following lines are equivalent:\n",
    "# d = a @ b @ [[10.], [11.]]\n",
    "# d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
    "\n",
    "# print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 理解静态shape和动态shape\n",
    "\n",
    "\n",
    "TensorFlow有两种shape，分别是：\n",
    "\n",
    "- 静态shape，在构建计算图的时候就已经知道了shape\n",
    "- 动态图，在实际的Session运行的时候才知道\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128)\n",
      "[None, 128]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.placeholder(tf.float32, [None,128])\n",
    "static_shape = a.shape\n",
    "print(static_shape)\n",
    "static_shape_list = static_shape.as_list()\n",
    "print(static_shape_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.shape`获取数据的动态shape，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dynamic_shape = tf.shape(a)\n",
    "print(dynamic_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用`set_shape`方法设置shape，静态动态都可以,前提必须是兼容的，也就是如果一个张量被定义成一个动态类型，可以被设置尺寸兼容的静态类型，然后就不能改了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 128]\n",
      "(32, 128)\n"
     ]
    }
   ],
   "source": [
    "a.set_shape([32,128])\n",
    "print(a.shape.as_list())\n",
    "a.set_shape([None, 128])\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 128]\n",
      "[32, 32]\n"
     ]
    }
   ],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 128])\n",
    "print(b.shape.as_list())\n",
    "b = tf.reshape(b, [32,32])\n",
    "print(b.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用`tf.reshape`方法进行修改shape，但是只是对动态类型进行reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写一个函数，用来获取张量的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "10\n",
      "20\n",
      "[<tf.Tensor 'unstack:0' shape=() dtype=int32>, 10, 20]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_shape(tensor):\n",
    "    static_shape = tensor.shape.as_list()\n",
    "    dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "    for s in zip(static_shape, dynamic_shape):\n",
    "        print(s[0])\n",
    "    \n",
    "    dims = [s[1] if s[0] is None else s[0]\n",
    "           for s in zip(static_shape, dynamic_shape)]\n",
    "    return dims\n",
    "\n",
    "b = tf.placeholder(tf.float32, [None, 10,20])\n",
    "shape = get_shape(b)\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 200]\n"
     ]
    }
   ],
   "source": [
    "# 这样就可以不管是动态的shape还是静态的shape，都可以使用reshape进行改造张量的shape了\n",
    "b = tf.reshape(b, [shape[0], shape[1] * shape[2]])\n",
    "print(b.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的作用就是，将动态shape和静态shape组合成对应的map，若是，动态shape，其对应的静态shape的值是None，那么返回一个tf的操作，对应的是这个动态的shape，否则，就返回真实的shape值，再利用重新组合的dims进行reshape的时候，不会因为None值造成计算的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Scopes以及何时使用它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const:0\n",
      "Variable:0\n"
     ]
    }
   ],
   "source": [
    "# 在TF中的张量和变量都有一个name属性，用于在计算图中识别对应的张量或者变量，如果没有显式的指定name值，那么就默认自动给一个值，如下\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1)\n",
    "print(a.name)\n",
    "\n",
    "b = tf.Variable(1)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of a is ConstantA:0\n",
      "The name of b is VariableB:0\n"
     ]
    }
   ],
   "source": [
    "# 指定名字\n",
    "a = tf.constant(1, name = \"ConstantA\")\n",
    "\n",
    "b = tf.Variable(1, name = \"VariableB\")\n",
    "\n",
    "print(\"The name of a is {0}\".format(a.name))\n",
    "print(\"The name of b is {0}\".format(b.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of a is scope/ConstantA:0\n",
      "the name of b is scope/VariableB:0\n",
      "the name of c is VariableC:0\n"
     ]
    }
   ],
   "source": [
    "# 在TF中，有两种方式改变张量和变量的名字，第一种是使用`tf.name_scope`\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope(\"scope\"):\n",
    "    a = tf.constant(1, name = \"ConstantA\")\n",
    "    print(\"the name of a is {0}\".format(a.name))\n",
    "    \n",
    "    b = tf.Variable(1, name = \"VariableB\")\n",
    "    print(\"the name of b is {0}\".format(b.name))\n",
    "    \n",
    "    c = tf.get_variable(name = \"VariableC\", shape=[])\n",
    "    print(\"the name of c is {0}\".format(c.name))\n",
    "    \n",
    "#     d = tf.get_variable(name = \"VariableC\", shape=[])\n",
    "#     ValueError: Variable VariableC already exists, disallowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 上面有两种创建变量的方法，一种是使用`tf.Variable`，另外一种是调用`tf.get_variable`，第二种方法根据提供的参数创建一个新的变量，如果这个变量已经存在了，就会返回一个`ValueError`的异常，提示不能重复声明变量。\n",
    "`tf.name_scope`影响使用`tf.Variable`创建的变量，不影响使用`tf.get_varible`创建的变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of a is scope/ConstantA:0\n",
      "the name of b is scope/VariableB:0\n",
      "the name of c is scope/VariableC:0\n"
     ]
    }
   ],
   "source": [
    "# 第二种方式，使用tf.variable_scope，也能修改使用tf.get_variable创建的变量\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"scope\"):\n",
    "    a = tf.constant(1, name=\"ConstantA\")\n",
    "    print(\"the name of a is {0}\".format(a.name))\n",
    "    \n",
    "    b = tf.Variable(1, name = \"VariableB\")\n",
    "    print(\"the name of b is {0}\".format(b.name))\n",
    "    \n",
    "    c = tf.get_variable(name=\"VariableC\", shape=[])\n",
    "    print(\"the name of c is {0}\".format(c.name))\n",
    "    \n",
    "#     d = tf.get_variable(name=\"VariableC\", shape=[]) \n",
    "#     ValueError: Variable scope/VariableC already exists, disallowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of a1 is scope/a:0\n",
      "the name of a2 is scope/a:0\n"
     ]
    }
   ],
   "source": [
    "# 但是当想要使用同一个变量的时候，`tf.variable_scope`也会允许使用相同的名字\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"scope\"):\n",
    "    a1 = tf.get_variable(name=\"a\", shape=[])\n",
    "with tf.variable_scope(\"scope\", reuse=True):\n",
    "    a2 = tf.get_variable(name = \"a\", shape=[])\n",
    "    \n",
    "print(\"the name of a1 is {0}\".format(a1.name))\n",
    "print(\"the name of a2 is {0}\".format(a2.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这样，对于使用内置的神经网络层例子变得很方便，但是还不是很理解这里，这里运行会出错。\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# image1 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "# image2 = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "# with tf.variable_scope(\"my_scope\"):\n",
    "#     features1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)\n",
    "# with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "#     features2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另外，tf.make_templace需要了解一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 broadcasting的好处和坏处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF支持元素对之间操作的broadcasting，正常情况下，把一个shape是[3,2]的张量与另外一个[3,1]的张量相加，或出错，但是TF会隐式的将少的哪一个维度进行拼接，然后再进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  3.]\n",
      " [ 5.  6.]]\n",
      "[[ 2.  3.]\n",
      " [ 5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant([[1.,2.],[3.,4.]])\n",
    "b = tf.constant([[1.],[2.]])\n",
    "\n",
    "c = a + b\n",
    "\n",
    "d = a + tf.tile(b,[1,2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Feeding Data\n",
    "TF被设计成用来高效的处理海量数据，所以不要让你的模型挨饿，有很多中往TF模型中填充数据的方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 常量\n",
    "常量是最简单的方式，将数据嵌入到模型中，但是这样做非常不灵活，这样的模型只能处理固定的数据，对于其他的数据，必须重新写这些模型，并且，这样的模型需要把整个数据库都添加到内存，但是实际一段时间内使用的总是那么其中的一小部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比如，将所有的数据都装入到内存\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "actual_data = np.random.normal(size=[100])\n",
    "data = tf.constant(actual_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 PlaceHolder\n",
    "用占位符的方式，解决上面提到的两个问题，占位符只是输入数据的占位，不是具体的数据，具体的数据是在Session运行的时候，通过feed_dict填充的，因此，使用占位符的方法，既可以解决数据写死的问题，也解决了将所有的数据都添加到内存的问题，因为，占位符是用过在feed_dict中寻找数据，并不是一次完全加载到内存，只是在需要的时候才加载到内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.2255888 ,  1.00040305,  5.16353798,  1.00296748,  1.00011981,\n",
       "        4.32047462,  2.17466545,  1.00011301,  2.33910155,  3.02162504,\n",
       "        5.73065996,  1.31538284,  6.34142876,  2.42048049,  1.00509977,\n",
       "        1.74918652,  1.26414847,  1.20581484,  1.05658484,  1.62368524,\n",
       "        1.0129739 ,  1.66226029,  3.58274817,  1.00007153,  1.10945976,\n",
       "        3.01686645,  2.01928377,  1.25830305,  1.17454302,  1.06953335,\n",
       "        1.96208167,  1.36904383,  1.00974679,  1.57326114,  1.06944907,\n",
       "        1.17518342,  1.00283849,  1.13761389,  1.23225224,  1.70099807,\n",
       "        1.5279355 ,  1.34609449,  2.74484396,  1.02827525,  1.03170788,\n",
       "        1.16090035,  5.57027531,  1.10606861,  1.29976952,  2.15356994,\n",
       "        1.00606954,  2.9145751 ,  2.62280512,  1.08841705,  4.2875061 ,\n",
       "        1.07244444,  2.19253826,  1.01574445,  4.37201118,  1.71269882,\n",
       "        1.47084546,  1.2036047 ,  1.06024444,  1.73561406,  3.2600863 ,\n",
       "        1.14203453,  4.56035948,  1.01300478,  1.94401479,  1.07738113,\n",
       "        2.77773285,  1.21865892,  1.05301952,  1.01948166,  1.06049478,\n",
       "        2.63663316,  1.02088177,  1.16861761,  1.0151974 ,  1.22824025,\n",
       "        2.1134243 ,  2.41645527,  1.32801986,  2.82443285,  2.14670658,\n",
       "        1.08716393,  3.03814602,  1.27870035,  2.68672633,  1.50039506,\n",
       "        3.13280129,  1.00000536,  1.10963273,  1.00507367,  1.20386159,\n",
       "        1.37332106,  3.24578476,  1.14254761,  2.19163609,  9.83730316], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = tf.placeholder(tf.float32)\n",
    "\n",
    "prediction = tf.square(data) + 1\n",
    "\n",
    "actual_data = np.random.normal(size=[100])\n",
    "\n",
    "tf.Session().run(prediction, feed_dict={data:actual_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 使用Python操作\n",
    "写一个产生或者读取数据的Python函数，然后将Python函数传入到TF，下面的是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def py_input_fn():\n",
    "    actual_data = np.random.normal(size[100])\n",
    "    return actual_data\n",
    "\n",
    "data = tf.py_func(py_input_fn, [], (tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 使用DataSet API\n",
    "建议使用TensorFlow进行读取数据是使用TensorFlow 的DataSet API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-59-d0de82a45446>:4: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib as tfc\n",
    "\n",
    "actual_data = np.random.normal(size=[100])\n",
    "dataset = tfc.data.Dataset.from_tensor_slices(actual_data)\n",
    "data = dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**如果是使用Dataset从文件中读取数据，那么最好是把数据写成TFrecord格式，使用TFRecordDataset读取这样格式的文件，效率会更高。比如下面的例子：\n",
    "```\n",
    "dataset = tf.contrib.data.TFRecordDataset(path_to_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset API让我们使用在pipeline中处理数据更加方便，下面的例子就是在获取dataset对象后如何进行数据处理。\n",
    "```\n",
    "dataset = ...\n",
    "dataset = dataset.cache()\n",
    "if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(batch_size * 5)\n",
    "    \n",
    "dataset = dataset.map(parse, num_threads = 8)\n",
    "dataset = dataset.batch(batch_size)\n",
    "```\n",
    "- 第一行数获取一个DataSet对象，不管是从变量中读取还是从文件中读取\n",
    "- 第二行数使用DataSet.cache方法把数据缓存在内存中，用来改善读取数据的性能\n",
    "- 第三行是判断是否在训练的模式\n",
    "- 第四行，若是在训练模式，重复dataset无限次，这样在训练过程中进行多次迭代\n",
    "- 第五行，重新洗牌数据，随机的从当前的数据库中选取batch_size大小的数据，然后随机放在新的数据库\n",
    "- 第六行，使用map对dataset对象中的数据逐行的处理，可能数数据格式转换等，parse就是一个map函数，用来将原始数据逐行的映射成新的数据库，后面的表示处理数据的线程数\n",
    "- 最后一行创建batch\n",
    "\n",
    "更多关于DataSet API的介绍参考官网的文档：[https://www.tensorflow.org/api_docs/python/tf/data/Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 利用好重载操作符\n",
    "\n",
    "与Numpy类似，TF重载了大量Python的操作符，这样在构建计算图更方便，代码也更适合阅读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20192504 -2.4070553   1.67237801 -0.36907138 -0.32653037  0.40492151\n",
      " -1.23739867 -0.56534736  0.17567957 -0.90513461]\n",
      "[ 0.20192504 -2.4070553   1.67237801 -0.36907138 -0.32653037  0.40492151\n",
      " -1.23739867 -0.56534736  0.17567957 -0.90513461]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = np.random.normal(size=100)\n",
    "\n",
    "begin = 0\n",
    "end = 10\n",
    "z = x[begin:end]\n",
    "tfz = tf.slice(x, begin=[begin], size=[end - begin])\n",
    "print(z)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tfz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**要小心使用这个操作符，分片操作符是非常低效的，而且尽量避免使用分片，特变是当分片的数量非常高的适合，下面的一个例子是在演示分片效率为啥低，将一个矩阵按照行的方向合并，最终合并成一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.365443 seconds.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "x = tf.random_uniform([500, 100])\n",
    "\n",
    "z = tf.zeros([100])\n",
    "\n",
    "for i in range(500):\n",
    "    z += x[i]\n",
    "\n",
    "sess = tf.Session()\n",
    "start = time.time();\n",
    "sess.run(z)\n",
    "print(\"Took %f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.097241 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 更好的方式使用，tf.unstack替代slice\n",
    "\n",
    "z = tf.zeros([100])\n",
    "for x_i in tf.unstack(x):\n",
    "    z += x_i\n",
    "    \n",
    "start = time.time();\n",
    "sess.run(z)\n",
    "print(\"Took %f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[-1 -2]\n",
      " [-3 -4]]\n",
      "[[ 6  8]\n",
      " [10 12]]\n",
      "[[-4 -4]\n",
      " [-4 -4]]\n",
      "[[ 5 12]\n",
      " [21 32]]\n",
      "[[ 0.2         0.33333333]\n",
      " [ 0.42857143  0.5       ]]\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[    1    64]\n",
      " [ 2187 65536]]\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "[[False False]\n",
      " [False False]]\n",
      "[[False False]\n",
      " [False False]]\n",
      "[[ True  True]\n",
      " [ True  True]]\n",
      "[[ True  True]\n",
      " [ True  True]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# 其他的操作符重载\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.constant([1,2,3,4], shape=[2,2])\n",
    "y = tf.constant([5,6,7,8], shape=[2,2])\n",
    "\n",
    "z = -x\n",
    "print(sess.run(x))\n",
    "print(sess.run(z))\n",
    "\n",
    "z = x + y\n",
    "print(sess.run(z))\n",
    "z = x - y\n",
    "print(sess.run(z))\n",
    "z = x * y\n",
    "print(sess.run(z))\n",
    "z = x / y\n",
    "print(sess.run(z))\n",
    "z = x // y\n",
    "print(sess.run(z))\n",
    "z = x % y\n",
    "print(sess.run(z))\n",
    "z = x ** y\n",
    "print(sess.run(z))\n",
    "z = x @ y\n",
    "print(sess.run(z))\n",
    "z = x > y\n",
    "print(sess.run(z))\n",
    "z = x >= y\n",
    "print(sess.run(z))\n",
    "z = x < y\n",
    "print(sess.run(z))\n",
    "z = x <= y\n",
    "print(sess.run(z))\n",
    "z = abs(x)\n",
    "print(sess.run(z))\n",
    "# z = x & y\n",
    "# print(sess.run(z))\n",
    "# z = x | y\n",
    "# print(sess.run(z))\n",
    "# z = x ^ y\n",
    "# print(sess.run(z))\n",
    "# z = ~x\n",
    "# print(sess.run(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**TF中没有重载操作符==和!=，这两个操作必须使用函数tf.equal() 和 tf.not_equal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 理解执行顺序和控制依赖\n",
    "TF的操作不会马上执行，而是，首先在计算图中创建节点，最后使用Session.run()函数执行这些操作。这种方式，允许在运行时优化执行顺序，或者去除没有用的节点。如果在计算图中只有张量，那么不用担心依赖问题，但是如果在计算图中还有Variable，就需要担心依赖的问题，计算图中存在Variable会把事情变得很复杂，这里的建议是，如果张量对于程序没有什么作用，就只使用Variable。这样说可能不会有什么感觉，通过下面的例子说明这些问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "\n",
    "a = a + b\n",
    "\n",
    "print(tf.Session().run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的程序将a和b的值进行加法运算，然后把结果存储在a中，这个程序实际上创建了三个张量，两个常量张量存储两个数值，另外一个张量存储结果，不能复写一个张量的值，如果需要修改一个张量的值，那么必须创建一个新的张量，就像上面程序一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**，在TF中，如果没有顶一个新的计算图，那么会自动创建一个计算图作为默认的计算图，可以使用`tf.get_default_graph()`函数获取这个计算图的句柄，然后检查这个计算图."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'Const_1:0' shape=() dtype=int32>, <tf.Tensor 'add:0' shape=() dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "print(tfc.graph_editor.get_tensors(tf.get_default_graph()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与张量不同，Variable是可以被更新的，下面是演示如何使用Variable在代码中完成基本的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(1)\n",
    "b = tf.Variable(2)\n",
    "\n",
    "assign = tf.assign(a , a + b)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(assign))\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的程序中可以看出，输出我们期望的结果，assign表示两个变量的加法操作，输出起操作的返回值是3，变量a的值也被修改成3了。这个例子也看不出什么来，下面一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n",
      "[5, 7]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(1)\n",
    "b = tf.Variable(2)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "assign = tf.assign(a , 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "for i in range(10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([assign, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果看出来，最终的加法结果出现了不同，原因是赋值操作与加法操作存在依赖。可能是定义的操作的顺序不同会影响结果，但是，真正需要关注的是控制依赖，也就是control dependencies，对于张量来说，控制依赖是直接的，只要在一个操作中使用了一个张量，就会定义一个隐式的对这个张量的依赖，但是当使用Variable的时候却更复杂，因为VariableTake更多的数值。\n",
    "因此，在使用Variable的时候，需要显示的调用函数`tf.control_dependencies`定义依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n",
      "[5, 3]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.Variable(1)\n",
    "b = tf.Variable(2)\n",
    "c = a + b\n",
    "\n",
    "with tf.control_dependencies([c]):\n",
    "    assgin = tf.assign(a, 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "for i in range(10):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([assgin, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法保证，在使用assign的时候，依赖c操作，assign执行必须在c之后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 控制流操作：条件与循环\n",
    "在使用TF创建一个复杂的模型的时候，比如RNN，就需要使用条件和循环控制操作。这一部分介绍常用的流操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 第一个，根据预测值，判断是否使用乘法或者加法\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "p = tf.constant(True)\n",
    "\n",
    "x = tf.cond(p, lambda: a + b, lambda : a * b)\n",
    "\n",
    "print(tf.Session().run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    }
   ],
   "source": [
    "# 第二个，在模型中通常使用大批量的张量或者矩阵处理，与cond相关的操作是where，比如下面的例子\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant([1, 1])\n",
    "b = tf.constant([2, 2])\n",
    "\n",
    "p = tf.constant([True, False])\n",
    "\n",
    "x = tf.where(p, a + b, a * b)\n",
    "\n",
    "print(tf.Session().run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# 第三个，另外一个常用的是while_loop，用来创建一个动态的循环，处理序列，比如处理斐波那契数列\n",
    "# while_loop接收一个条件函数，一个循环体函数，另外是初始化的参数值用来初始化循环变量，\n",
    "# 循环变量是只要条件函数满足，就会持续调用循环体函数更新循环变量\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n = tf.constant(5)\n",
    "\n",
    "\n",
    "def cond(i, a, b):\n",
    "    return i < n\n",
    "\n",
    "\n",
    "def body(i, a, b):\n",
    "    return i + 1, b, a + b\n",
    "\n",
    "\n",
    "i, a, b = tf.while_loop(cond, body, (2, 1, 1))\n",
    "\n",
    "print(tf.Session().run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# 需要打印整个斐波那契数列的时候，需要在while_loop中设置shape_invariants参数\n",
    "# 但是这个写循环体有些丑陋，并且效率很低，很多中间变量不会被使用\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n = tf.constant(5)\n",
    "\n",
    "\n",
    "def cond(i, a, b, c):\n",
    "    return i < n\n",
    "\n",
    "\n",
    "def body(i, a, b, c):\n",
    "    return i + 1, b, a + b, tf.concat([c, [a + b]], 0)\n",
    "\n",
    "\n",
    "i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, tf.constant([1,1])),\n",
    "                           shape_invariants=(tf.TensorShape([]),\n",
    "                                             tf.TensorShape([]),\n",
    "                                             tf.TensorShape([]),\n",
    "                                             tf.TensorShape([None])))\n",
    "\n",
    "print(tf.Session().run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# 这次使用的是tf.TensorArray\n",
    "# 可以尝试使用while_loop实现Beam Search\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n = tf.constant(5)\n",
    "\n",
    "c = tf.TensorArray(tf.int32, n)\n",
    "c = c.write(0, 1)\n",
    "c = c.write(1, 1)\n",
    "\n",
    "\n",
    "def cond(i, a, b, c):\n",
    "    return i < n\n",
    "\n",
    "\n",
    "def body(i, a, b, c):\n",
    "    c = c.write(i, a + b)\n",
    "    return i + 1, b, a + b, c\n",
    "\n",
    "\n",
    "i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, c))\n",
    "c = c.stack()\n",
    "\n",
    "print(tf.Session().run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 原型内核与Python操作的原型可视化\n",
    "在TF中的操作核都是完全用C++语言实现的，因为C++语言的高效性，但是完全用C++ 语言实现TF kernel回事很痛苦的，所以，在花费大量的时间实现字节的内核的时候，最好快速的实现一个原型，这个原型可能是效率比较低的。使用`tf.py_func`函数，可以将Python的代码片段转换成TF的操作。\n",
    "\n",
    "下面的一段代码演示如何使用Python语言实现一个简单的ReLU非线性核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.78949737549e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import uuid\n",
    "\n",
    "def relu(inputs):\n",
    "\n",
    "    # 使用Python语言定义操作\n",
    "    def _relu(x):\n",
    "        return np.maximum(x, 0.)\n",
    "    # 在Python中定义操作的梯度\n",
    "    def _relu_grad(x):\n",
    "        return np.float32(x > 0)\n",
    "\n",
    "    # 一个适配器，这个适配器定义了与TF兼容的操作\n",
    "    def _relu_grad_op(op, grad):\n",
    "        x = op.inputs[0]\n",
    "        x_grad = grad * tf.py_func(_relu_grad, [x], tf.float32)\n",
    "        return x_grad\n",
    "\n",
    "    # 使用unique id注册梯度\n",
    "    grad_name = \"MyReluGrad_\" + str(uuid.uuid4())\n",
    "    tf.RegisterGradient(grad_name)(_relu_grad_op)\n",
    "\n",
    "    # 使用我么自定的梯度，复写梯度\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\":grad_name}):\n",
    "        output = tf.py_func(_relu, [inputs], tf.float32)\n",
    "    return output\n",
    "\n",
    "# 测试一下\n",
    "x = tf.random_uniform([10])\n",
    "y = relu(x  * x)\n",
    "\n",
    "with tf.Session():\n",
    "    diff = tf.test.compute_gradient_error(x, [10], y, [10])\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 这种实现方法非常低效的，而且只是用来做原型才有用，因为Python语言不能并行化，也不能运行在GPU上。为了确定自己的idea，需要写C++语言的kernel。\n",
    "实际上，我们通常使用python语言在TensorBoard上进行可视化操作。考虑这样一个例子，构建一个分类的模型，想在这个模型训练过程中可视化预测结果。但是TensorFlow提供了一个可视化图像方法`tf.summary.image()`。\n",
    "```\n",
    "image = tf.placeholder(tf.float32)\n",
    "tf.summary.image(\"image\",image)\n",
    "```\n",
    "但是这只能可视化输入的图像，为了可视化预测结果，必须寻找其他的方式给图像添加注解，但是根据存在的操作，是不可能实现的，有一个比较容易的方法，就是在python中完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_labeled_images(images, labels, max_outputs=3, name=\"image\"):\n",
    "    def _visualize_image(image, label):\n",
    "        fig = plt.figure(figsize=(3, 3), dpi=80)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(image[::-1, ...])\n",
    "        ax.text(0, 0, str(label),\n",
    "                hotizontalalignment=\"left\",\n",
    "                verticalalignment=\"top\"\n",
    "                )\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        data = fig.savefig(buf, format=\"png\")\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Read the image and convert to numpy array\n",
    "        img = PIL.Image.open(buf)\n",
    "        return np.array(img.getdata()).reshape(img.size[0], img.size[1], -1)\n",
    "\n",
    "    def _visualize_images(images, labels):\n",
    "        # Only display the given number of examples in the batch\n",
    "        outputs = []\n",
    "        for i in range(max_outputs):\n",
    "            output = _visualize_image(images[i], labels[i])\n",
    "            outputs.append(output)\n",
    "        return np.array(outputs, dtype=np.uint8)\n",
    "\n",
    "        # Run the python op.\n",
    "\n",
    "    figs = tf.py_func(_visualize_images, [images, labels], tf.uint8)\n",
    "    return tf.summary.image(name, figs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于这样的summary只有在一次循环结束后才记性，不是每一步都进行这样的操作，所以不用担心他们的效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 多GPU数据并行化\n",
    "如果一个程序是在单一cpu核心上运行，那么要想让他在多个GPU平行运行，可能需要从头开始重新写代码。但是在TF中不是这样的，由于他们的符号特性，TensorFlow能够银行这些复杂，并不需要多少工作就扩展你的程序到多个GPU或者多个CPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91057229,  1.00892568,  0.45219362, ...,  1.86331069,\n",
       "         0.46488905,  1.45825863],\n",
       "       [ 0.57930422,  1.01266611,  0.38713729, ...,  1.03819656,\n",
       "         1.10744143,  1.30509627],\n",
       "       [ 1.03566146,  0.73183095,  0.55242896, ...,  0.34626365,\n",
       "         0.37053537,  0.79335165],\n",
       "       ..., \n",
       "       [ 1.1578145 ,  1.17446208,  1.71888876, ...,  1.77600384,\n",
       "         1.12935531,  1.35291541],\n",
       "       [ 0.55161846,  0.64391768,  0.76529121, ...,  1.64219666,\n",
       "         1.56293046,  0.27215552],\n",
       "       [ 1.14429069,  0.60297847,  0.53495836, ...,  1.50707364,\n",
       "         0.78703678,  1.45006239]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个张量之间的加法，在一个CPU上运行\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(tf.DeviceSpec(device_type=\"CPU\", device_index=0)):\n",
    "    a = tf.random_uniform([1000,100])\n",
    "    b = tf.random_uniform([1000, 100])\n",
    "    c = a+b\n",
    "\n",
    "tf.Session().run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个张量子啊一个GPU上进行加法\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=0)):\n",
    "    a = tf.random_uniform([1000, 100])\n",
    "    b = tf.random_uniform([1000, 100])\n",
    "    c = a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用两个GPU进行并行处理\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a = tf.random_uniform([1000, 100])\n",
    "b = tf.random_uniform([1000, 100])\n",
    "\n",
    "split_a = tf.split(a, 2)\n",
    "split_b = tf.split(b, 2)\n",
    "\n",
    "split_c = []\n",
    "for i in range(2):\n",
    "    with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "        split_c.append(split_a[i] + split_b[i])\n",
    "\n",
    "c = tf.concat(split_c, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个更通用的例子\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a = tf.random_uniform([1000, 100])\n",
    "b = tf.random_uniform([1000, 100])\n",
    "\n",
    "\n",
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                out_split.append(fn(**{k: v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    return tf.concat(out_split, axis=0)\n",
    "\n",
    "\n",
    "def model(a, b):\n",
    "    return a + b\n",
    "\n",
    "\n",
    "c = make_parallel(model, 2, a=a, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想取代其他的操作，只需要替换Model函数中操作，这个Model可以接受多个张量作为参数，并范湖一个张量作为结果，在运行过程中，输入的张量和输出的张量都被分割成小片，然后批量的处理，在下一个例子中这种操作变得很方便。\n",
    "\n",
    "再看一个更具有实用价值的例子，我们想要在多个GPU上训练网络，在训练过程中，不仅要进行前向的计算，还要计算反向输出，但是怎样并行计算梯度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.45736778],\n",
      "       [ 0.32305717],\n",
      "       [-0.1650344 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib as tfc\n",
    "\n",
    "tf.reset_default_graph()\n",
    "a = tf.random_uniform([1000, 100])\n",
    "b = tf.random_uniform([1000, 100])\n",
    "\n",
    "\n",
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    out_split = []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "                out_split.append(fn(**{k: v[i] for k, v in in_splits.items()}))\n",
    "\n",
    "    return tf.concat(out_split, axis=0)\n",
    "\n",
    "\n",
    "def model(x, y):\n",
    "    w = tf.get_variable(\"w\", shape=[3, 1])\n",
    "\n",
    "    f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "    yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "\n",
    "    loss = tf.square(yhat - y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "loss = model(x, y)\n",
    "# loss = make_parallel(model, 1, x=x, y=y)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(tf.reduce_sum(loss), colocate_gradients_with_ops=True)\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.normal(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "\n",
    "_, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})\n",
    "print(sess.run(tfc.framework.get_variables_by_name(\"w\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实用并行处理，需要修改两个地方\n",
    "loss = make_parallel(model, 1, x=x,y=y)\n",
    "train_op = tf.train.AdamOptimizer(0.01).minimize(tf.reduce_sum(loss), colocate_gradients_with_ops=True)\n",
    "# 最后一行表示，保证梯度计算在同一个device上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 调试TF模型\n",
    "TF的符号本性让调试TF模型相对常规的python代码困难一些，这一节介绍一些调试TF模型的工具\n",
    "\n",
    "在TF中，最常见的错误是在传递张量的时候，shape可能是不匹配的，很多TF的操作符，能处理不同ranks和shape的张量，这在使用API的时候很方便，但是也会导致在寻找错误的时候很头疼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 两个矩阵相乘\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.random_uniform([2,3])\n",
    "b = tf.random_uniform([3,4])\n",
    "c = tf.matmul(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(10, 2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码也是两个矩阵相乘，但是批量的相乘，只进行最内两层的矩阵相乘\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.random_uniform([10, 2, 3])\n",
    "b = tf.random_uniform([10, 3, 4])\n",
    "c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(2, 2), dtype=float32)\n",
      "[[ 2.  3.]\n",
      " [ 3.  4.]]\n"
     ]
    }
   ],
   "source": [
    "# 还有就是TF的broadcasting的功能，自动扩展张量\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "c = a + b\n",
    "print(c)\n",
    "print(tf.Session().run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 使用断言操作验证张量\n",
    "一种降低非期望行为的方法是显示的验证中间张量的rank和shape。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 断言跟其他的节点是一样的，如果被剪枝的的话，不能评估，所以需要显示的定义依赖\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "\n",
    "# check_a = tf.assert_rank(a, 1) # 这里应该是1，但是为了能够运行通过，写成了2\n",
    "check_a = tf.assert_rank(a, 2)\n",
    "check_b = tf.assert_rank(b, 1)\n",
    "\n",
    "with tf.control_dependencies([check_a, check_b]):\n",
    "    c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多断言在官网上有说明：[https://www.tensorflow.org/api_guides/python/check_ops](https://www.tensorflow.org/api_guides/python/check_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 使用tf.Print 用日志的方法记录tensor的值\n",
    "另外一种使用的内置调试方法是tf.Print,这个方法将给定的张量用日志的方式记录到标准错误\n",
    "```\n",
    "input_copy = tf.Print(input, tensors_to_print_list)\n",
    "```\n",
    "\n",
    "**注意：**tf.Print函数返回第一个参数的拷贝作为输出，强制tf.Print运行的一种方式是把它传递给另外一个操作符，并且执行，下面的这个例子是在两个张量加法操作之前打印日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[1.], [2.]])\n",
    "b = tf.constant([1., 2.])\n",
    "\n",
    "check_all = tf.Print(a, [a, b])\n",
    "c = a + b\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 使用tf.compute_gradient_error检查梯度\n",
    "所有的在TF的操作都会有梯度，很容易没有意识到构建一个计算图，对于TF不能计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.358309    0.09924446  0.14590074  0.0956795   0.30086628]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def no_differenctiable_softmax_entropy(logits):\n",
    "    probs = tf.nn.softmax(logits=logits)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=probs,logits=logits)\n",
    "\n",
    "w = tf.get_variable(\"w\", shape=[5])\n",
    "y = -no_differenctiable_softmax_entropy(w)\n",
    "\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(y)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(10000):\n",
    "    sess.run(train_op)\n",
    "\n",
    "print(sess.run(tf.nn.softmax(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子，使用tf.nn.softmax_cross_entroy_with_logits定义一个分类分布的熵，使用Adam优化算子寻找最小的熵，在信息论里面知道，当方差为0的时候，熵最小，这里对应的每一类的权重都相等的时候，熵最小，即，结果应该是[0.2,0.2,0.2,0.2,0.2],但是实际上的结果不一样。\n",
    "\n",
    "原因是使用tf.nn.softmax_cross_entroy_with_logits没有定义与标签的梯度。\n",
    "\n",
    "但是当我们不能直接确定这个原因的时候，怎么才能确定问题呢？\n",
    "\n",
    "幸运的是TF提供一个数值查分算子，能够用来寻找符号梯度误差。\n",
    "```\n",
    "with tf.Session():\n",
    "    diff = tf.test.compute_gradient_error(w,[5],y,[])\n",
    "    print(diff)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例子会出错，暂时不放了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
